"""
Speed Boost Script for Stable Diffusion WebUI

This script implements various optimization techniques to significantly speed up
image generation with minimal quality loss, based on proven optimization methods.

Optimizations included:
- FP16 (Half Precision) for faster computation and reduced memory usage
- Inference mode for better performance by disabling autograd tracking
- Memory optimization techniques
- Optional advanced optimizations
"""

import gradio as gr
import torch
import modules.scripts as scripts
from modules import shared, devices, script_callbacks
from modules.processing import StableDiffusionProcessing
from modules.script_callbacks import CFGDenoiserParams
import logging
from typing import Any, Dict, List, Optional, Union
import contextlib
import time
import json
import gc
import os

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Console output function for debug information
def debug_print(message: str, show_debug: bool = True):
    """Print debug information to console when debug mode is enabled"""
    if show_debug:
        # Print to console
        print(f"[SpeedBoost] {message}")
        # Also log to logger for additional visibility
        logger.info(f"SpeedBoost: {message}")
        # Try to print to shared progress output if available
        if hasattr(shared, 'progress_print_out') and shared.progress_print_out:
            print(f"[SpeedBoost] {message}", file=shared.progress_print_out)

class SpeedBoostScript(scripts.Script):
    """Speed Boost optimization script for faster image generation"""
    
    def __init__(self):
        super().__init__()
        self.alwayson = True
        self.original_dtype = None
        self.optimization_active = False
        self.start_time = None
        self.performance_stats = {
            'total_generations': 0,
            'optimized_generations': 0,
            'average_speedup': 0.0,
            'best_speedup': 0.0,
            'total_time_saved': 0.0
        }
        
    def title(self):
        return "Speed Boost"
    
    def show(self, is_img2img):
        return scripts.AlwaysVisible
    
    def ui(self, is_img2img):
        """Create the Speed Boost UI components"""
        
        with gr.Group():
            with gr.Accordion("Speed Boost Settings", open=False):
                # Main enable/disable checkbox
                enable_speedboost = gr.Checkbox(
                    label="Enable Speed Boost",
                    value=False,
                    info="Enable optimizations for faster image generation"
                )
                
                # Conservative mode toggle
                conservative_mode = gr.Checkbox(
                    label="Conservative Mode (Recommended)",
                    value=True,
                    info="Safer optimizations that avoid potential compatibility issues"
                )
                
                with gr.Row():
                    with gr.Column():
                        # FP16 optimization
                        use_fp16 = gr.Checkbox(
                            label="Use FP16 (Half Precision)",
                            value=False,  # Default to False for safety
                            info="Use 16-bit floating point for ~60% speed increase (may cause errors with some models)"
                        )
                        
                        # Inference mode optimization
                        use_inference_mode = gr.Checkbox(
                            label="Use Inference Mode",
                            value=True,
                            info="Disable autograd tracking for better performance"
                        )
                        
                    with gr.Column():
                        # Memory optimization
                        optimize_memory = gr.Checkbox(
                            label="Optimize Memory Usage",
                            value=True,
                            info="Reduce memory footprint for larger batches"
                        )
                        
                        # Advanced optimizations
                        advanced_opts = gr.Checkbox(
                            label="Advanced Optimizations",
                            value=False,
                            info="Enable experimental optimizations (may vary by GPU)"
                        )
                
                # JavaScript to handle conservative mode
                conservative_mode.change(
                    fn=None,
                    inputs=[conservative_mode],
                    outputs=[use_fp16, advanced_opts],
                    _js="""(conservative) => {
                        if (conservative) {
                            return [false, false];  // Disable FP16 and advanced opts in conservative mode
                        } else {
                            return [false, false];  // Keep current values when not conservative
                        }
                    }"""
                )
                
                # Information display
                with gr.Row():
                    info_text = gr.HTML(
                        value="""
                        <div style='padding: 10px; background-color: #1f2937; border-radius: 8px; margin-top: 10px;'>
                            <h4 style='color: #10b981; margin: 0 0 8px 0;'>ðŸš€ Speed Boost Optimizations</h4>
                            <div style='padding: 8px; background-color: #065f46; border-radius: 6px; margin-bottom: 10px;'>
                                <p style='margin: 0; color: #10b981; font-weight: bold;'>âœ… Fixed: Data type mismatch errors</p>
                                <p style='margin: 4px 0 0 0; font-size: 0.9em; color: #d1fae5;'>Conservative mode prevents BFloat16/Half precision conflicts</p>
                            </div>
                            <div style='padding: 8px; background-color: #1e40af; border-radius: 6px; margin-bottom: 10px;'>
                                <p style='margin: 0; color: #60a5fa; font-weight: bold;'>ðŸš€ NEW: Intelligent Step Optimization for All Step Counts!</p>
                                <p style='margin: 4px 0 0 0; font-size: 0.9em; color: #dbeafe;'>Adaptive reduction works with any step count (5-100+) while maintaining quality</p>
                            </div>
                            <ul style='margin: 0; padding-left: 20px; color: #d1d5db;'>
                                <li><strong>Conservative Mode:</strong> Safe optimizations + gentle step reduction (3-25% based on step count)</li>
                                <li><strong>FP16:</strong> Uses half-precision for ~60% speed increase (disable if errors occur)</li>
                                <li><strong>Inference Mode:</strong> Disables autograd + aggressive step optimization (10-50% reduction)</li>
                                <li><strong>Memory Optimization:</strong> Reduces memory usage + enables step optimization</li>
                                <li><strong>Advanced:</strong> GPU-specific optimizations + maximum step reduction</li>
                            </ul>
                            <p style='margin: 8px 0 0 0; font-size: 0.9em; color: #9ca3af;'>
                                ðŸ’¡ Enable "Speed Boost" first, then choose your optimization level. Step optimization adapts to ANY step count!
                            </p>
                        </div>
                        """
                    )
                
                # Debug information
                show_debug = gr.Checkbox(
                    label="Show Debug Information",
                    value=False,
                    info="Display optimization status in console"
                )
                
                # Performance statistics display
                with gr.Row():
                    perf_display = gr.HTML(
                        value=self._get_performance_html(),
                        label="Performance Statistics"
                    )
                    
                    refresh_stats = gr.Button(
                        value="ðŸ”„ Refresh Stats",
                        size="sm"
                    )
                
                # Update performance display when refresh is clicked
                refresh_stats.click(
                    fn=lambda: self._get_performance_html(),
                    outputs=[perf_display]
                )
        
        return [
            enable_speedboost,
            conservative_mode,
            use_fp16,
            use_inference_mode,
            optimize_memory,
            advanced_opts,
            show_debug
        ]
    
    def process(self, p: StableDiffusionProcessing, 
                enable_speedboost: bool,
                conservative_mode: bool,
                use_fp16: bool,
                use_inference_mode: bool,
                optimize_memory: bool,
                advanced_opts: bool,
                show_debug: bool):
        """Apply speed optimizations before processing with enhanced error handling"""
        
        # DEBUG: Always print to console to verify script is being called
        print(f"[SPEEDBOOST DEBUG] process() called with args: enable={enable_speedboost}, conservative={conservative_mode}, fp16={use_fp16}, inference={use_inference_mode}, memory={optimize_memory}, advanced={advanced_opts}, debug={show_debug}")
        
        # Start timing for performance measurement
        self.start_time = time.time()
        
        if not enable_speedboost:
            print(f"[SPEEDBOOST DEBUG] Speed Boost disabled - all optimizations skipped")
            debug_print("Speed Boost disabled - all optimizations skipped", show_debug)
            return
        
        try:
            # Enhanced validation and error prevention
            if not hasattr(shared, 'sd_model') or shared.sd_model is None:
                debug_print("No model loaded, skipping Speed Boost optimizations", show_debug)
                return
            
            # Validate processing parameters to prevent NoneType errors
            if p is None:
                debug_print("Processing parameters are None, skipping optimizations", show_debug)
                return
            
            self.optimization_active = True
            
            debug_print("Applying optimizations with enhanced validation...", show_debug)
            
            # Early comprehensive precision validation and fixing
            debug_print("Running comprehensive model validation...", show_debug)
            precision_report = self._validate_model_precision()
            if 'BFloat16_vs_Half' in precision_report.get('conflicts', []):
                debug_print("BFloat16/Half conflicts detected in validation, applying fixes...", show_debug)
                self._fix_all_precision_conflicts(show_debug)
        
        except Exception as validation_e:
            debug_print(f"Validation failed: {validation_e}, continuing with basic optimizations", show_debug)
            self.optimization_active = True
        
        # Store original settings for restoration
        if not hasattr(self, '_original_settings_stored'):
            self._store_original_settings()
            self._original_settings_stored = True
        
        # Apply conservative mode overrides
        if conservative_mode:
            use_fp16 = False  # Disable FP16 in conservative mode
            advanced_opts = False  # Disable advanced optimizations in conservative mode
            debug_print("Conservative mode active - FP16 and advanced optimizations disabled", show_debug)
        
        # Store optimization settings for performance tracking
        p.speedboost_settings = {
            'conservative_mode': conservative_mode,
            'fp16': use_fp16,
            'inference_mode': use_inference_mode,
            'memory_opt': optimize_memory,
            'advanced_opts': advanced_opts,
            'debug': show_debug
        }
        
        # Store original steps for progress calculation fix
        p.speedboost_original_steps = p.steps
        
        # Apply optimizations with individual error handling
        try:
            # Apply FP16 optimization
            if use_fp16:
                self._apply_fp16_optimization(p, show_debug)
        except Exception as fp16_e:
            debug_print(f"FP16 optimization failed: {fp16_e}", show_debug)
        
        try:
            # Apply inference mode optimization
            if use_inference_mode:
                self._apply_inference_mode(p, show_debug)
        except Exception as inference_e:
            debug_print(f"Inference mode optimization failed: {inference_e}", show_debug)
        
        try:
            # Apply memory optimizations
            if optimize_memory:
                self._apply_memory_optimization(p, show_debug)
        except Exception as memory_e:
            debug_print(f"Memory optimization failed: {memory_e}", show_debug)
        
        try:
            # Apply advanced optimizations
            if advanced_opts:
                self._apply_advanced_optimizations(p, show_debug)
        except Exception as advanced_e:
            debug_print(f"Advanced optimizations failed: {advanced_e}", show_debug)
            
        try:
            # Apply dynamic step optimization for faster inference
            # Dynamic step optimization is safe even in conservative mode as it doesn't involve precision changes
            print(f"[SPEEDBOOST DEBUG] Step optimization decision: inference_mode={use_inference_mode}, advanced_opts={advanced_opts}, optimize_memory={optimize_memory}, conservative_mode={conservative_mode}")
            if use_inference_mode or advanced_opts or optimize_memory:
                print(f"[SPEEDBOOST DEBUG] Calling dynamic step optimization")
                debug_print(f"Using dynamic step optimization (inference_mode={use_inference_mode}, advanced_opts={advanced_opts}, optimize_memory={optimize_memory})", show_debug)
                self._apply_dynamic_step_optimization(p, show_debug)
            elif conservative_mode:
                # In conservative mode, apply a gentler version of step optimization
                print(f"[SPEEDBOOST DEBUG] Calling conservative step optimization")
                debug_print(f"Using conservative step optimization (conservative_mode={conservative_mode})", show_debug)
                self._apply_conservative_step_optimization(p, show_debug)
            else:
                print(f"[SPEEDBOOST DEBUG] No step optimization applied - no optimization flags enabled")
                debug_print("No step optimization applied - no optimization flags enabled", show_debug)
        except Exception as dynamic_e:
            print(f"[SPEEDBOOST DEBUG] Step optimization failed: {dynamic_e}")
            debug_print(f"Step optimization failed: {dynamic_e}", show_debug)
        
        except Exception as process_e:
            debug_print(f"Process method encountered an error: {process_e}", show_debug)
            # Ensure cleanup in case of errors
            if hasattr(self, 'optimization_active'):
                self.optimization_active = False
        
        # Fix total progress calculation if step optimization was applied
        try:
            if (hasattr(p, 'speedboost_original_steps') and 
                hasattr(shared, 'total_tqdm') and hasattr(shared, 'state')):
                if p.steps != p.speedboost_original_steps:
                    # Calculate the correct total steps for batch processing
                    if hasattr(p, 'enable_hr') and p.enable_hr:
                        # For hires fix: (first_pass_steps + hr_steps) * job_count
                        hr_steps = getattr(p, 'hr_second_pass_steps', None) or p.steps
                        total_steps = (p.steps + hr_steps) * shared.state.job_count
                    else:
                        # For normal processing: steps * job_count
                        total_steps = p.steps * shared.state.job_count
                    
                    # Update the total progress bar
                    shared.total_tqdm.updateTotal(total_steps)
                    
                    if show_debug:
                        print(f"[SPEEDBOOST DEBUG] Updated total progress: {p.speedboost_original_steps} -> {p.steps} steps, total: {total_steps}")
        except Exception as progress_e:
            debug_print(f"Failed to update total progress: {progress_e}", show_debug)
        
        debug_print("All optimizations applied successfully", show_debug)
    
    def _store_original_settings(self):
        """Store original settings for restoration"""
        try:
            # Store original model dtype if available
            if hasattr(shared.sd_model, 'dtype'):
                self.original_dtype = shared.sd_model.dtype
                
            # Store original VAE dtype if available
            if hasattr(shared.sd_model, 'first_stage_model') and hasattr(shared.sd_model.first_stage_model, 'dtype'):
                self.original_vae_dtype = shared.sd_model.first_stage_model.dtype
                
            # Store original text encoder dtype if available
            if hasattr(shared.sd_model, 'cond_stage_model') and hasattr(shared.sd_model.cond_stage_model, 'dtype'):
                self.original_te_dtype = shared.sd_model.cond_stage_model.dtype
                
        except Exception as e:
            logger.warning(f"Speed Boost: Could not store original dtype: {e}")
    
    def _restore_original_precision(self, debug: bool = False):
        """Restore original model precision"""
        try:
            # Restore main model precision
            if hasattr(self, 'original_dtype') and hasattr(shared.sd_model, 'to'):
                if shared.sd_model.dtype != self.original_dtype:
                    shared.sd_model = shared.sd_model.to(self.original_dtype)
                    if debug:
                        logger.info(f"Speed Boost: Restored main model to {self.original_dtype}")
            
            # Restore VAE precision
            if (hasattr(self, 'original_vae_dtype') and 
                hasattr(shared.sd_model, 'first_stage_model') and 
                hasattr(shared.sd_model.first_stage_model, 'to')):
                try:
                    if shared.sd_model.first_stage_model.dtype != self.original_vae_dtype:
                        shared.sd_model.first_stage_model = shared.sd_model.first_stage_model.to(self.original_vae_dtype)
                        if debug:
                            logger.info(f"Speed Boost: Restored VAE to {self.original_vae_dtype}")
                except Exception as vae_e:
                    if debug:
                        logger.warning(f"Speed Boost: VAE precision restoration failed: {vae_e}")
            
            # Restore text encoder precision
            if (hasattr(self, 'original_te_dtype') and 
                hasattr(shared.sd_model, 'cond_stage_model') and 
                hasattr(shared.sd_model.cond_stage_model, 'to')):
                try:
                    if shared.sd_model.cond_stage_model.dtype != self.original_te_dtype:
                        shared.sd_model.cond_stage_model = shared.sd_model.cond_stage_model.to(self.original_te_dtype)
                        if debug:
                            logger.info(f"Speed Boost: Restored text encoder to {self.original_te_dtype}")
                except Exception as te_e:
                    if debug:
                        logger.warning(f"Speed Boost: Text encoder precision restoration failed: {te_e}")
                        
        except Exception as e:
            logger.warning(f"Speed Boost: Precision restoration failed: {e}")
    
    def _apply_fp16_optimization(self, p: StableDiffusionProcessing, debug: bool):
        """Apply FP16 (half precision) optimization for faster computation with data type consistency"""
        try:
            debug_print("Applying FP16 optimization with data type consistency...", debug)
            
            # Early precision conflict detection and resolution
            debug_print("Performing early precision conflict detection...", debug)
            
            # Check for BFloat16 vs Half conflicts that cause the specific error
            if self._has_bfloat16_half_conflict():
                debug_print("Critical BFloat16/Half conflicts detected, resolving before optimization...", debug)
                self._fix_all_precision_conflicts(debug)
                # Force garbage collection after precision fixes
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                debug_print("Precision conflicts resolved, continuing with optimization...", debug)
            
            # Check for reForge compatibility first
            if hasattr(shared.sd_model, 'forge_objects'):
                debug_print("reForge detected - using conservative precision handling", debug)
                self._apply_reforge_safe_precision(p, debug)
                return
            
            # Store original data types for restoration
            original_dtypes = {}
            
            # Apply FP16 to main model with consistency checks
            if hasattr(shared.sd_model, 'half') and shared.sd_model.dtype != torch.float16:
                original_dtypes['main_model'] = shared.sd_model.dtype
                shared.sd_model = shared.sd_model.half()
                debug_print(f"Main model converted to FP16 (was {original_dtypes['main_model']})", debug)
            
            # Apply FP16 to VAE with consistency checks
            if hasattr(shared.sd_model, 'first_stage_model'):
                try:
                    if hasattr(shared.sd_model.first_stage_model, 'half'):
                        original_dtypes['vae'] = getattr(shared.sd_model.first_stage_model, 'dtype', None)
                        shared.sd_model.first_stage_model = shared.sd_model.first_stage_model.half()
                        
                        # Ensure all VAE components use the same precision
                        self._ensure_vae_precision_consistency(debug)
                        debug_print("VAE converted to FP16 with consistency checks", debug)
                except Exception as vae_e:
                    debug_print(f"VAE FP16 conversion failed: {vae_e}", debug)
            
            # Apply FP16 to text encoder if available
            if hasattr(shared.sd_model, 'cond_stage_model'):
                try:
                    if hasattr(shared.sd_model.cond_stage_model, 'half'):
                        original_dtypes['text_encoder'] = getattr(shared.sd_model.cond_stage_model, 'dtype', None)
                        shared.sd_model.cond_stage_model = shared.sd_model.cond_stage_model.half()
                        debug_print("Text encoder converted to FP16", debug)
                except Exception as te_e:
                    debug_print(f"Text encoder FP16 conversion failed: {te_e}", debug)
            
            # Store original dtypes for restoration
            p.speedboost_original_dtypes = original_dtypes
            
            # Set torch backend to use optimized attention if available
            if hasattr(torch.backends.cuda, 'enable_math_sdp'):
                torch.backends.cuda.enable_math_sdp(True)
                debug_print("Enabled CUDA math SDP for attention optimization", debug)
                
        except Exception as e:
            debug_print(f"FP16 optimization failed: {e}", debug)
    
    def _is_fp16_safe(self) -> bool:
        """Check if FP16 optimization is safe for the current environment"""
        try:
            # Check if CUDA is available and supports FP16
            if not torch.cuda.is_available():
                return False
            
            # Check GPU capability (FP16 works best on modern GPUs)
            if torch.cuda.get_device_capability()[0] < 6:  # Pascal or newer
                return False
            
            # Check for known problematic model configurations
            if self._has_mixed_precision_issues():
                return False
            
            # Check if model has problematic mixed precision components
            if hasattr(shared.sd_model, 'forge_objects'):
                # reForge specific checks - be more conservative
                return self._check_reforge_compatibility()
            
            return True
            
        except Exception:
            return False
    
    def _has_mixed_precision_issues(self) -> bool:
        """Check for known mixed precision issues with enhanced detection"""
        try:
            # Comprehensive precision validation
            precision_report = self._validate_model_precision()
            
            # Check if VAE and main model have different precisions
            if hasattr(shared.sd_model, 'first_stage_model'):
                main_dtype = getattr(shared.sd_model, 'dtype', None)
                vae_dtype = getattr(shared.sd_model.first_stage_model, 'dtype', None)
                
                if main_dtype and vae_dtype and main_dtype != vae_dtype:
                    debug_print(f"Mixed precision detected: Main={main_dtype}, VAE={vae_dtype}", True)
                    return True
            
            # Check for reForge VAE precision issues
            if hasattr(shared.sd_model, 'forge_objects'):
                forge_objects = shared.sd_model.forge_objects
                if hasattr(forge_objects, 'vae'):
                    vae = forge_objects.vae
                    if hasattr(vae, 'first_stage_model'):
                        vae_dtype = getattr(vae.first_stage_model, 'dtype', None)
                        main_dtype = getattr(shared.sd_model, 'dtype', None)
                        if vae_dtype and main_dtype and vae_dtype != main_dtype:
                            debug_print(f"reForge mixed precision detected: Main={main_dtype}, VAE={vae_dtype}", True)
                            return True
            
            # Check for BFloat16 vs Half conflicts specifically
            if self._has_bfloat16_half_conflict():
                debug_print("BFloat16 vs Half conflict detected", True)
                return True
            
            return False
            
        except Exception as e:
            debug_print(f"Precision check failed: {e}", True)
            return True  # Assume issues if we can't check
    
    def _validate_model_precision(self) -> Dict[str, Any]:
        """Comprehensive model precision validation"""
        report = {
            'main_model': None,
            'vae': None,
            'text_encoder': None,
            'unet': None,
            'conflicts': []
        }
        
        try:
            # Check main model
            if hasattr(shared.sd_model, 'dtype'):
                report['main_model'] = shared.sd_model.dtype
            
            # Check VAE
            if hasattr(shared.sd_model, 'first_stage_model'):
                report['vae'] = getattr(shared.sd_model.first_stage_model, 'dtype', None)
            
            # Check text encoder
            if hasattr(shared.sd_model, 'cond_stage_model'):
                report['text_encoder'] = getattr(shared.sd_model.cond_stage_model, 'dtype', None)
            
            # Check UNet
            if hasattr(shared.sd_model, 'unet'):
                report['unet'] = getattr(shared.sd_model.unet, 'dtype', None)
            
            # Check for reForge specific components
            if hasattr(shared.sd_model, 'forge_objects'):
                forge_objects = shared.sd_model.forge_objects
                if hasattr(forge_objects, 'vae') and hasattr(forge_objects.vae, 'first_stage_model'):
                    forge_vae_dtype = getattr(forge_objects.vae.first_stage_model, 'dtype', None)
                    if forge_vae_dtype:
                        report['forge_vae'] = forge_vae_dtype
            
            # Detect conflicts
            dtypes = [v for v in report.values() if v is not None and isinstance(v, torch.dtype)]
            unique_dtypes = set(dtypes)
            
            if len(unique_dtypes) > 1:
                # Check for problematic combinations
                if torch.bfloat16 in unique_dtypes and torch.float16 in unique_dtypes:
                    report['conflicts'].append('BFloat16_vs_Half')
                if torch.float32 in unique_dtypes and torch.float16 in unique_dtypes:
                    report['conflicts'].append('Float32_vs_Half')
            
        except Exception as e:
            report['error'] = str(e)
        
        return report
    
    def _has_bfloat16_half_conflict(self) -> bool:
        """Specifically check for BFloat16 vs Half conflicts that cause the error"""
        try:
            # Check VAE components for mixed BFloat16/Half precision
            if hasattr(shared.sd_model, 'first_stage_model'):
                vae = shared.sd_model.first_stage_model
                
                # Check for conv layers with mismatched weight/bias types
                for name, module in vae.named_modules():
                    if isinstance(module, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):
                        if hasattr(module, 'weight') and hasattr(module, 'bias') and module.bias is not None:
                            weight_dtype = module.weight.dtype
                            bias_dtype = module.bias.dtype
                            
                            # Check for the specific conflict from the error
                            if (weight_dtype == torch.bfloat16 and bias_dtype == torch.float16) or \
                               (weight_dtype == torch.float16 and bias_dtype == torch.bfloat16):
                                debug_print(f"BFloat16/Half conflict in {name}: weight={weight_dtype}, bias={bias_dtype}", True)
                                return True
            
            # Check reForge VAE
            if hasattr(shared.sd_model, 'forge_objects') and hasattr(shared.sd_model.forge_objects, 'vae'):
                forge_vae = shared.sd_model.forge_objects.vae
                if hasattr(forge_vae, 'first_stage_model'):
                    for name, module in forge_vae.first_stage_model.named_modules():
                        if isinstance(module, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):
                            if hasattr(module, 'weight') and hasattr(module, 'bias') and module.bias is not None:
                                weight_dtype = module.weight.dtype
                                bias_dtype = module.bias.dtype
                                
                                if (weight_dtype == torch.bfloat16 and bias_dtype == torch.float16) or \
                                   (weight_dtype == torch.float16 and bias_dtype == torch.bfloat16):
                                    debug_print(f"reForge BFloat16/Half conflict in {name}: weight={weight_dtype}, bias={bias_dtype}", True)
                                    return True
            
            return False
            
        except Exception as e:
            debug_print(f"BFloat16/Half conflict check failed: {e}", True)
            return True  # Assume conflict if we can't check
    
    def _check_reforge_compatibility(self) -> bool:
        """Check reForge specific compatibility"""
        try:
            # reForge often uses mixed precision by default
            # Be conservative and only allow FP16 if explicitly safe
            
            # Check if the model is already optimized by reForge
            if hasattr(shared.sd_model, 'forge_objects'):
                # If reForge has already optimized the model, don't interfere
                return False
            
            return True
            
        except Exception:
            return False
    
    def _apply_reforge_safe_precision(self, p: StableDiffusionProcessing, debug: bool):
        """Apply safe precision handling for reForge models to avoid data type conflicts"""
        try:
            debug_print("Applying reForge-safe precision optimizations...", debug)
            
            # For reForge, avoid aggressive precision changes that can cause conflicts
            # Instead, focus on non-precision optimizations
            
            # Check current model precision
            if hasattr(shared.sd_model, 'forge_objects'):
                forge_objects = shared.sd_model.forge_objects
                
                # Get current VAE precision to avoid conflicts
                vae_dtype = None
                if hasattr(forge_objects, 'vae') and hasattr(forge_objects.vae, 'first_stage_model'):
                    vae_dtype = getattr(forge_objects.vae.first_stage_model, 'dtype', None)
                    debug_print(f"reForge VAE current dtype: {vae_dtype}", debug)
                
                # Only apply precision changes if they're consistent
                if vae_dtype == torch.float16:
                    debug_print("reForge VAE already in FP16 - maintaining consistency", debug)
                elif vae_dtype == torch.bfloat16:
                    debug_print("reForge VAE using BFloat16 - avoiding precision conflicts", debug)
                    # Don't force FP16 conversion to avoid BFloat16 vs Half conflicts
                else:
                    debug_print("reForge VAE in full precision - applying conservative optimizations", debug)
            
            # Apply non-precision optimizations that are safe for reForge
            self._apply_reforge_safe_optimizations(p, debug)
            
        except Exception as e:
            debug_print(f"reForge safe precision handling failed: {e}", debug)
    
    def _apply_reforge_safe_optimizations(self, p: StableDiffusionProcessing, debug: bool):
        """Apply optimizations that are safe for reForge without precision conflicts"""
        try:
            # Enable memory optimizations that don't affect precision
            if torch.cuda.is_available():
                torch.backends.cudnn.benchmark = True
                debug_print("Enabled cuDNN benchmark (reForge safe)", debug)
                
                # Enable TF32 without precision conversion
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                debug_print("Enabled TF32 optimizations (reForge safe)", debug)
            
        except Exception as e:
            debug_print(f"reForge safe optimizations failed: {e}", debug)
    
    def _ensure_vae_precision_consistency(self, debug: bool):
        """Ensure all VAE components use consistent data types to prevent conflicts"""
        try:
            # First, run comprehensive precision conflict detection and fixing
            debug_print("Running comprehensive precision conflict resolution...", debug)
            
            # Check for BFloat16 vs Half conflicts specifically
            if self._has_bfloat16_half_conflict():
                debug_print("BFloat16/Half conflicts detected, applying fixes...", debug)
                self._fix_all_precision_conflicts(debug)
            
            # Standard VAE precision consistency
            if not hasattr(shared.sd_model, 'first_stage_model'):
                return
            
            vae = shared.sd_model.first_stage_model
            target_dtype = torch.float16  # Our target precision
            
            # Check and fix common VAE components
            components_to_check = [
                'encoder', 'decoder', 'post_quant_conv', 'quant_conv'
            ]
            
            for component_name in components_to_check:
                if hasattr(vae, component_name):
                    component = getattr(vae, component_name)
                    if hasattr(component, 'half'):
                        try:
                            # Ensure component uses the same precision
                            component = component.half()
                            setattr(vae, component_name, component)
                            debug_print(f"VAE {component_name} precision aligned to FP16", debug)
                        except Exception as comp_e:
                            debug_print(f"Failed to align VAE {component_name} precision: {comp_e}", debug)
            
            # Enhanced precision fixing for conv layers
            self._fix_vae_conv_precision(vae, debug)
            
            # Specifically target post_quant_conv layer that commonly causes BFloat16/Half conflicts
            self._fix_post_quant_conv_precision(vae, debug)
            
            # Additional reForge VAE handling
            if hasattr(shared.sd_model, 'forge_objects') and hasattr(shared.sd_model.forge_objects, 'vae'):
                forge_vae = shared.sd_model.forge_objects.vae
                if hasattr(forge_vae, 'first_stage_model'):
                    debug_print("Applying precision consistency to reForge VAE...", debug)
                    self._fix_vae_conv_precision(forge_vae.first_stage_model, debug)
                    self._fix_post_quant_conv_precision(forge_vae.first_stage_model, debug)
            
        except Exception as e:
            debug_print(f"VAE precision consistency check failed: {e}", debug)
    
    def _fix_vae_conv_precision(self, vae, debug: bool):
        """Fix convolution layer precision mismatches in VAE with comprehensive handling"""
        try:
            fixed_count = 0
            
            # First pass: determine the safest target dtype
            target_dtype = self._determine_safe_vae_dtype(vae)
            debug_print(f"Target VAE dtype determined: {target_dtype}", debug)
            
            # Recursively check all modules for precision mismatches
            for name, module in vae.named_modules():
                if isinstance(module, (torch.nn.Conv2d, torch.nn.ConvTranspose2d, torch.nn.Linear)):
                    # Fix weight precision
                    if hasattr(module, 'weight') and module.weight.dtype != target_dtype:
                        module.weight.data = module.weight.data.to(target_dtype)
                        fixed_count += 1
                        debug_print(f"Fixed weight precision in {name}: -> {target_dtype}", debug)
                    
                    # Fix bias precision
                    if hasattr(module, 'bias') and module.bias is not None and module.bias.dtype != target_dtype:
                        module.bias.data = module.bias.data.to(target_dtype)
                        fixed_count += 1
                        debug_print(f"Fixed bias precision in {name}: -> {target_dtype}", debug)
                
                # Fix normalization layers
                elif isinstance(module, (torch.nn.BatchNorm2d, torch.nn.GroupNorm, torch.nn.LayerNorm)):
                    if hasattr(module, 'weight') and module.weight is not None and module.weight.dtype != target_dtype:
                        module.weight.data = module.weight.data.to(target_dtype)
                        fixed_count += 1
                    
                    if hasattr(module, 'bias') and module.bias is not None and module.bias.dtype != target_dtype:
                        module.bias.data = module.bias.data.to(target_dtype)
                        fixed_count += 1
            
            if fixed_count > 0:
                debug_print(f"Fixed {fixed_count} VAE precision mismatches to {target_dtype}", debug)
            
        except Exception as e:
            debug_print(f"VAE conv precision fix failed: {e}", debug)
    
    def _fix_post_quant_conv_precision(self, vae, debug: bool):
        """Specifically fix post_quant_conv layer precision conflicts"""
        try:
            if hasattr(vae, 'post_quant_conv'):
                post_quant_conv = vae.post_quant_conv
                debug_print(f"Checking post_quant_conv precision: weight={post_quant_conv.weight.dtype if hasattr(post_quant_conv, 'weight') else 'None'}, bias={post_quant_conv.bias.dtype if hasattr(post_quant_conv, 'bias') and post_quant_conv.bias is not None else 'None'}", debug)
                
                # Force both weight and bias to fp16 to prevent BFloat16/Half conflicts
                if hasattr(post_quant_conv, 'weight'):
                    if post_quant_conv.weight.dtype != torch.float16:
                        post_quant_conv.weight.data = post_quant_conv.weight.data.to(torch.float16)
                        debug_print(f"Fixed post_quant_conv weight: {post_quant_conv.weight.dtype} -> fp16", debug)
                
                if hasattr(post_quant_conv, 'bias') and post_quant_conv.bias is not None:
                    if post_quant_conv.bias.dtype != torch.float16:
                        post_quant_conv.bias.data = post_quant_conv.bias.data.to(torch.float16)
                        debug_print(f"Fixed post_quant_conv bias: {post_quant_conv.bias.dtype} -> fp16", debug)
                
                debug_print("post_quant_conv precision conflicts resolved", debug)
            
            # Also check quant_conv for similar issues
            if hasattr(vae, 'quant_conv'):
                quant_conv = vae.quant_conv
                if hasattr(quant_conv, 'weight') and quant_conv.weight.dtype != torch.float16:
                    quant_conv.weight.data = quant_conv.weight.data.to(torch.float16)
                    debug_print("Fixed quant_conv weight precision", debug)
                
                if hasattr(quant_conv, 'bias') and quant_conv.bias is not None and quant_conv.bias.dtype != torch.float16:
                    quant_conv.bias.data = quant_conv.bias.data.to(torch.float16)
                    debug_print("Fixed quant_conv bias precision", debug)
                    
        except Exception as e:
            debug_print(f"post_quant_conv precision fix failed: {e}", debug)
    
    def _determine_safe_vae_dtype(self, vae_model) -> torch.dtype:
        """Determine the safest dtype for VAE based on current state and compatibility"""
        try:
            # Count dtypes in the model
            dtype_counts = {}
            
            for name, module in vae_model.named_modules():
                if hasattr(module, 'weight') and module.weight is not None:
                    dtype = module.weight.dtype
                    dtype_counts[dtype] = dtype_counts.get(dtype, 0) + 1
            
            # Determine safest dtype
            if not dtype_counts:
                return torch.float16  # Default fallback
            
            # Prefer float16 over bfloat16 for stability in reForge
            if torch.float16 in dtype_counts:
                return torch.float16
            elif torch.bfloat16 in dtype_counts:
                # Only use bfloat16 if it's the only option and we're sure it's supported
                if self._is_bfloat16_safe():
                    return torch.bfloat16
                else:
                    return torch.float16
            elif torch.float32 in dtype_counts:
                # Convert float32 to float16 for memory efficiency
                return torch.float16
            else:
                # Use the most common dtype
                return max(dtype_counts.items(), key=lambda x: x[1])[0]
            
        except Exception as e:
            debug_print(f"Safe dtype determination failed: {e}", True)
            return torch.float16  # Safe fallback
    
    def _is_bfloat16_safe(self) -> bool:
        """Check if bfloat16 is safe to use in current environment"""
        try:
            # Check CUDA capability
            if torch.cuda.is_available():
                device_capability = torch.cuda.get_device_capability()
                # bfloat16 requires compute capability 8.0+
                if device_capability[0] >= 8:
                    return True
            
            # Check if we're in a reForge environment (more conservative)
            if hasattr(shared.sd_model, 'forge_objects'):
                return False  # Be conservative with reForge
            
            return False
            
        except Exception:
            return False
    
    def _fix_all_precision_conflicts(self, debug: bool = False) -> bool:
        """Comprehensive precision conflict resolution"""
        try:
            fixed_any = False
            
            # Fix main VAE
            if hasattr(shared.sd_model, 'first_stage_model'):
                self._fix_vae_conv_precision(shared.sd_model.first_stage_model, debug)
                fixed_any = True
            
            # Fix reForge VAE
            if hasattr(shared.sd_model, 'forge_objects') and hasattr(shared.sd_model.forge_objects, 'vae'):
                forge_vae = shared.sd_model.forge_objects.vae
                if hasattr(forge_vae, 'first_stage_model'):
                    self._fix_vae_conv_precision(forge_vae.first_stage_model, debug)
                    fixed_any = True
            
            # Fix UNet if needed
            if hasattr(shared.sd_model, 'unet'):
                if self._fix_unet_precision_conflicts(shared.sd_model.unet, debug):
                    fixed_any = True
            
            # Fix text encoder if needed
            if hasattr(shared.sd_model, 'cond_stage_model'):
                if self._fix_text_encoder_precision(shared.sd_model.cond_stage_model, debug):
                    fixed_any = True
            
            return fixed_any
            
        except Exception as e:
            debug_print(f"Comprehensive precision fix failed: {e}", debug)
            return False
    
    def _fix_unet_precision_conflicts(self, unet_model, debug: bool) -> bool:
        """Fix UNet precision conflicts"""
        try:
            fixed_count = 0
            target_dtype = torch.float16  # Safe default for UNet
            
            for name, module in unet_model.named_modules():
                if isinstance(module, (torch.nn.Conv2d, torch.nn.ConvTranspose2d, torch.nn.Linear)):
                    if hasattr(module, 'weight') and module.weight.dtype != target_dtype:
                        module.weight.data = module.weight.data.to(target_dtype)
                        fixed_count += 1
                    
                    if hasattr(module, 'bias') and module.bias is not None and module.bias.dtype != target_dtype:
                        module.bias.data = module.bias.data.to(target_dtype)
                        fixed_count += 1
            
            if fixed_count > 0:
                debug_print(f"Fixed {fixed_count} UNet precision conflicts", debug)
                return True
            
            return False
            
        except Exception as e:
            debug_print(f"UNet precision fix failed: {e}", debug)
            return False
    
    def _fix_text_encoder_precision(self, text_encoder, debug: bool) -> bool:
        """Fix text encoder precision conflicts"""
        try:
            fixed_count = 0
            target_dtype = torch.float16  # Safe default
            
            for name, module in text_encoder.named_modules():
                if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):
                    if hasattr(module, 'weight') and module.weight.dtype != target_dtype:
                        module.weight.data = module.weight.data.to(target_dtype)
                        fixed_count += 1
                    
                    if hasattr(module, 'bias') and module.bias is not None and module.bias.dtype != target_dtype:
                        module.bias.data = module.bias.data.to(target_dtype)
                        fixed_count += 1
            
            if fixed_count > 0:
                debug_print(f"Fixed {fixed_count} text encoder precision conflicts", debug)
                return True
            
            return False
            
        except Exception as e:
            debug_print(f"Text encoder precision fix failed: {e}", debug)
            return False
    
    def _convert_vae_to_fp16(self, debug: bool):
        """Safely convert VAE to FP16 with enhanced compatibility checks"""
        try:
            # Handle different VAE architectures in reForge
            if hasattr(shared.sd_model, 'forge_objects') and hasattr(shared.sd_model.forge_objects, 'vae'):
                # reForge VAE handling - be more conservative
                vae = shared.sd_model.forge_objects.vae
                if hasattr(vae, 'first_stage_model'):
                    current_dtype = getattr(vae.first_stage_model, 'dtype', None)
                    debug_print(f"reForge VAE current dtype: {current_dtype}", debug)
                    
                    # Only convert if it's safe and beneficial
                    if current_dtype not in [torch.float16, torch.bfloat16]:
                        if hasattr(vae.first_stage_model, 'half'):
                            vae.first_stage_model = vae.first_stage_model.half()
                            debug_print("Converted reForge VAE to FP16", debug)
                    else:
                        debug_print(f"reForge VAE already optimized ({current_dtype}), skipping conversion", debug)
            elif hasattr(shared.sd_model, 'first_stage_model'):
                # Standard VAE handling
                if hasattr(shared.sd_model.first_stage_model, 'half'):
                    shared.sd_model.first_stage_model = shared.sd_model.first_stage_model.half()
                    # Ensure precision consistency
                    self._ensure_vae_precision_consistency(debug)
                    debug_print("Converted standard VAE to FP16 with consistency checks", debug)
        except Exception as e:
            debug_print(f"VAE FP16 conversion failed: {e}", debug)
    
    def _convert_text_encoder_to_fp16(self, debug: bool):
        """Safely convert text encoder to FP16"""
        try:
            # Handle text encoder conversion
            if hasattr(shared.sd_model, 'cond_stage_model'):
                if hasattr(shared.sd_model.cond_stage_model, 'half'):
                    shared.sd_model.cond_stage_model = shared.sd_model.cond_stage_model.half()
                    if debug:
                        logger.info("Speed Boost: Converted text encoder to FP16")
        except Exception as e:
            if debug:
                logger.warning(f"Speed Boost: Text encoder FP16 conversion failed: {e}")
    
    def _apply_inference_mode(self, p: StableDiffusionProcessing, debug: bool):
        """Apply inference mode optimization"""
        try:
            debug_print("Applying inference mode optimization...", debug)
            
            # Set inference mode flag for the processing
            p.speedboost_inference_mode = True
            
            # Enable torch.no_grad() context for inference
            if hasattr(torch, 'inference_mode'):
                p.speedboost_use_inference_mode = True
                debug_print("Torch inference mode will be applied during generation", debug)
            else:
                debug_print("Torch inference mode not available, using no_grad instead", debug)
                
        except Exception as e:
            debug_print(f"Inference mode setup failed: {e}", debug)
    
    def _apply_memory_optimization(self, p: StableDiffusionProcessing, debug: bool):
        """Apply memory optimization techniques with enhanced batch processing support"""
        try:
            debug_print("Applying memory optimizations...", debug)
            
            # Clear CUDA cache before optimization
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                debug_print("Cleared CUDA cache", debug)
            
            # Enable memory efficient attention if available
            if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
                # PyTorch 2.0+ has native memory efficient attention
                p.speedboost_use_sdpa = True
                debug_print("Enabled Scaled Dot Product Attention (SDPA)", debug)
            
            # Enable channels last memory format for better performance
            try:
                if hasattr(shared.sd_model, 'to'):
                    # Convert to channels last memory format
                    shared.sd_model = shared.sd_model.to(memory_format=torch.channels_last)
                    debug_print("Applied channels last memory format", debug)
            except Exception as mem_e:
                debug_print(f"Channels last conversion failed: {mem_e}", debug)
            
            # Optimize memory allocation with batch processing considerations
            if torch.cuda.is_available():
                # Calculate appropriate memory fraction based on batch settings
                batch_size = getattr(p, 'batch_size', 1)
                n_iter = getattr(p, 'n_iter', 1)
                total_images = batch_size * n_iter
                
                # Adjust memory fraction based on total images to prevent OOM
                if total_images > 10:
                    memory_fraction = 0.8  # More conservative for large batches
                elif total_images > 5:
                    memory_fraction = 0.85
                else:
                    memory_fraction = 0.9
                
                try:
                    torch.cuda.set_per_process_memory_fraction(memory_fraction)
                    debug_print(f"Set CUDA memory fraction to {memory_fraction*100:.0f}% for {total_images} total images", debug)
                except:
                    pass
                
                # Enable memory pool for better allocation patterns
                try:
                    if hasattr(torch.cuda, 'memory_pool'):
                        torch.cuda.memory_pool().set_memory_fraction(memory_fraction)
                except:
                    pass
                
                # Enable additional memory optimizations for batch processing
                try:
                    # Set memory allocator settings for better batch handling
                    if hasattr(torch.cuda, 'set_memory_strategy'):
                        torch.cuda.set_memory_strategy('native')
                except:
                    pass
            
            # Apply model-specific memory optimizations
            try:
                if hasattr(shared.sd_model, 'model') and hasattr(shared.sd_model.model, 'diffusion_model'):
                    # Enable gradient checkpointing if available (saves memory during training/inference)
                    diffusion_model = shared.sd_model.model.diffusion_model
                    if hasattr(diffusion_model, 'enable_gradient_checkpointing'):
                        diffusion_model.enable_gradient_checkpointing()
                        debug_print("Enabled gradient checkpointing", debug)
            except Exception as gc_e:
                debug_print(f"Gradient checkpointing failed: {gc_e}", debug)
            
            # Force garbage collection
            gc.collect()
            
            # Additional cleanup for batch processing
            if torch.cuda.is_available():
                # Synchronize CUDA operations
                torch.cuda.synchronize()
                # Clear cache again after optimizations
                torch.cuda.empty_cache()
            
            debug_print("Applied memory optimizations successfully", debug)
                
        except Exception as e:
            debug_print(f"Memory optimization failed: {e}", debug)
    
    def _apply_advanced_optimizations(self, p: StableDiffusionProcessing, debug: bool):
        """Apply cutting-edge optimization techniques based on latest research"""
        try:
            debug_print("Applying research-based advanced optimizations...", debug)
            
            if torch.cuda.is_available():
                # Enable TensorFloat-32 (TF32) on Ampere GPUs
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                debug_print("Enabled TF32 for faster matrix operations", debug)
                
                # Enable cuDNN benchmark mode for consistent input sizes
                torch.backends.cudnn.benchmark = True
                debug_print("Enabled cuDNN benchmark mode", debug)
                
                # Apply channels_last memory format for better performance (with reForge compatibility)
                try:
                    # Be more conservative with reForge models
                    if hasattr(shared.sd_model, 'forge_objects'):
                        debug_print("reForge detected - skipping channels_last to avoid precision conflicts", debug)
                    else:
                        if hasattr(shared.sd_model, 'unet') and hasattr(shared.sd_model.unet, 'to'):
                            shared.sd_model.unet = shared.sd_model.unet.to(memory_format=torch.channels_last)
                            debug_print("Applied channels_last memory format to UNet", debug)
                        
                        if hasattr(shared.sd_model, 'first_stage_model') and hasattr(shared.sd_model.first_stage_model, 'to'):
                            shared.sd_model.first_stage_model = shared.sd_model.first_stage_model.to(memory_format=torch.channels_last)
                            debug_print("Applied channels_last memory format to VAE", debug)
                except Exception as mem_e:
                    debug_print(f"Channels_last conversion failed: {mem_e}", debug)
                
                # Enable optimized CUDA attention backends (FlashAttention v2, memory-efficient attention)
                try:
                    # Enable FlashAttention v2 and memory-efficient attention
                    torch.backends.cuda.enable_flash_sdp(True)
                    torch.backends.cuda.enable_mem_efficient_sdp(True)
                    torch.backends.cuda.enable_math_sdp(True)  # Fallback
                    
                    # Try to enable FlashAttention v2 if available (up to 44% faster than xformers)
                    try:
                        import flash_attn
                        debug_print("FlashAttention v2 detected - will provide up to 44% speedup over xformers", debug)
                    except ImportError:
                        debug_print("FlashAttention v2 not available, using PyTorch SDPA backends", debug)
                    
                    debug_print("Enabled optimized CUDA attention backends (FlashAttention v2, memory-efficient SDP)", debug)
                except Exception as e:
                    debug_print(f"CUDA attention backend setup failed: {e}", debug)
                
                # Apply torch.compile optimizations for PyTorch 2.0+ with reForge compatibility
                try:
                    if hasattr(torch, 'compile') and torch.__version__ >= '2.0':
                        # Check for reForge compatibility before applying torch.compile
                        if hasattr(shared.sd_model, 'forge_objects'):
                            debug_print("reForge detected - using conservative torch.compile settings", debug)
                            # Use minimal compile settings for reForge compatibility
                            compile_mode = "default"  # More conservative than reduce-overhead
                        else:
                            # Configure inductor for maximum performance based on latest research
                            torch._inductor.config.conv_1x1_as_mm = True
                            torch._inductor.config.coordinate_descent_tuning = True
                            torch._inductor.config.epilogue_fusion = True  # Re-enabled for better fusion
                            torch._inductor.config.coordinate_descent_check_all_directions = True
                            torch._inductor.config.triton.unique_kernel_names = True
                            torch._inductor.config.triton.cudagraphs = True
                            torch._inductor.config.fx_graph_cache = True  # Enable graph caching
                            
                            # Use reduce-overhead mode for better performance in inference
                            compile_mode = "reduce-overhead"  # Better than max-autotune for inference
                        
                        # Compile UNet for maximum speed (with reForge safety checks)
                        if hasattr(shared.sd_model, 'unet') and not hasattr(shared.sd_model.unet, '_speedboost_compiled'):
                            # Skip UNet compilation for reForge to avoid conflicts
                            if not hasattr(shared.sd_model, 'forge_objects'):
                                shared.sd_model.unet = torch.compile(
                                    shared.sd_model.unet, 
                                    mode=compile_mode, 
                                    fullgraph=False,  # More stable than fullgraph=True
                                    dynamic=False
                                )
                                shared.sd_model.unet._speedboost_compiled = True
                                debug_print(f"Compiled UNet with torch.compile ({compile_mode})", debug)
                            else:
                                debug_print("Skipping UNet compilation for reForge compatibility", debug)
                        
                        # Skip VAE decoder compilation for reForge to avoid precision conflicts
                        if hasattr(shared.sd_model, 'first_stage_model') and hasattr(shared.sd_model.first_stage_model, 'decode'):
                            if not hasattr(shared.sd_model.first_stage_model.decode, '_speedboost_compiled'):
                                if not hasattr(shared.sd_model, 'forge_objects'):
                                    shared.sd_model.first_stage_model.decode = torch.compile(
                                        shared.sd_model.first_stage_model.decode,
                                        mode=compile_mode,
                                        fullgraph=False,
                                        dynamic=False
                                    )
                                    shared.sd_model.first_stage_model.decode._speedboost_compiled = True
                                    debug_print("Compiled VAE decoder with torch.compile", debug)
                                else:
                                    debug_print("Skipping VAE decoder compilation for reForge compatibility", debug)
                        
                        p.speedboost_torch_compiled = True
                        
                except Exception as compile_e:
                    debug_print(f"Torch compile optimization failed: {compile_e}", debug)
                
                # Enable CUDA graphs and advanced CUDA optimizations
                try:
                    if hasattr(torch.cuda, 'CUDAGraph') and torch.cuda.get_device_capability()[0] >= 7:
                        p.speedboost_use_cuda_graph = True
                        debug_print("CUDA graphs enabled for inference", debug)
                    
                    # Enable additional CUDA optimizations based on research
                    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed
                    torch.backends.cudnn.allow_tf32 = True
                    torch.backends.cuda.enable_math_sdp(True)
                    
                    # Set optimal matmul precision for speed
                    if hasattr(torch, 'set_float32_matmul_precision'):
                        torch.set_float32_matmul_precision('high')  # Enables fast matrix multiplication
                        debug_print("Set float32 matmul precision to 'high' for faster computation", debug)
                    
                    debug_print("Enabled CUDA graphs, TF32, and advanced CUDA optimizations", debug)
                except Exception as e:
                    debug_print(f"CUDA optimization setup failed: {e}", debug)
                
                # Optimize CUDA kernel launches and memory allocation
                try:
                    torch.cuda.set_sync_debug_mode(0)  # Disable sync debugging
                    if hasattr(torch.cuda, 'memory_pool'):
                        torch.cuda.set_per_process_memory_fraction(0.9)
                    debug_print("Optimized CUDA kernel launches and memory", debug)
                except:
                    pass
            
            # Enable CPU optimizations
            try:
                # Set optimal number of threads based on hardware
                if hasattr(torch, 'set_num_threads'):
                    num_threads = min(8, os.cpu_count() or 4)
                    torch.set_num_threads(num_threads)
                    debug_print(f"Set torch threads to {num_threads}", debug)
                
                # Enable optimized CPU kernels
                if hasattr(torch.backends, 'mkldnn'):
                    torch.backends.mkldnn.enabled = True
                    debug_print("Enabled MKL-DNN CPU optimizations", debug)
            except Exception as cpu_e:
                debug_print(f"CPU optimizations failed: {cpu_e}", debug)
            
            # Apply Winograd convolution optimization for 3x3 kernels
            try:
                if hasattr(torch.backends.cudnn, 'allow_tf32'):
                    torch.backends.cudnn.allow_tf32 = True
                    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed
                    debug_print("Enabled Winograd and non-deterministic optimizations", debug)
            except:
                pass
            
            # Apply model-specific optimizations
            self._apply_model_specific_optimizations(p, debug)
            
            debug_print("Research-based advanced optimizations applied successfully", debug)
                
        except Exception as e:
            debug_print(f"Advanced optimizations failed: {e}", debug)
    
    def _apply_model_specific_optimizations(self, p: StableDiffusionProcessing, debug: bool):
        """Apply model-specific optimizations for Stable Diffusion"""
        try:
            debug_print("Applying model-specific optimizations...", debug)
            
            # Optimize attention patterns for Stable Diffusion
            if hasattr(shared.sd_model, 'model') and hasattr(shared.sd_model.model, 'diffusion_model'):
                unet = shared.sd_model.model.diffusion_model
                
                # Apply attention optimization to all attention layers
                for name, module in unet.named_modules():
                    if 'attn' in name.lower() or 'attention' in name.lower():
                        try:
                            # Enable memory efficient attention if available
                            if hasattr(module, 'set_use_memory_efficient_attention_xformers'):
                                module.set_use_memory_efficient_attention_xformers(True)
                            elif hasattr(module, 'set_processor'):
                                # Use PyTorch 2.0 SDPA processor
                                try:
                                    from diffusers.models.attention_processor import AttnProcessor2_0
                                    module.set_processor(AttnProcessor2_0())
                                except ImportError:
                                    pass
                        except Exception:
                            continue
                
                debug_print("Applied attention optimizations to UNet layers", debug)
            
            # Enable Winograd convolution optimization for supported layers
            try:
                torch.backends.cudnn.allow_tf32 = True
                torch.backends.cudnn.benchmark = True  # Enable cuDNN autotuner
                debug_print("Enabled Winograd convolution and cuDNN autotuner", debug)
            except Exception as e:
                debug_print(f"Convolution optimization failed: {e}", debug)
            
            # Apply VAE-specific optimizations
            if hasattr(shared.sd_model, 'first_stage_model'):
                vae = shared.sd_model.first_stage_model
                try:
                    # Enable sliced attention for VAE if available
                    if hasattr(vae, 'enable_slicing'):
                        vae.enable_slicing()
                        debug_print("Enabled VAE sliced attention", debug)
                    
                    # Enable tiling for large images
                    if hasattr(vae, 'enable_tiling'):
                        vae.enable_tiling()
                        debug_print("Enabled VAE tiling for memory efficiency", debug)
                        
                except Exception as e:
                    debug_print(f"VAE optimization failed: {e}", debug)
            
            # Set optimal thread count for CPU operations
            try:
                import os
                cpu_count = os.cpu_count() or 4
                optimal_threads = min(cpu_count, 8)  # Cap at 8 for optimal performance
                torch.set_num_threads(optimal_threads)
                debug_print(f"Set optimal CPU thread count: {optimal_threads}", debug)
            except Exception as e:
                debug_print(f"CPU thread optimization failed: {e}", debug)
                
        except Exception as e:
            debug_print(f"Model-specific optimizations failed: {e}", debug)
    
    def _apply_dynamic_step_optimization(self, p: StableDiffusionProcessing, debug: bool):
        """Apply dynamic step reduction based on research findings"""
        try:
            print(f"[SPEEDBOOST DEBUG] _apply_dynamic_step_optimization called")
            debug_print("Applying dynamic step optimization...", debug)
            
            # Get current step count
            original_steps = getattr(p, 'steps', 20)
            print(f"[SPEEDBOOST DEBUG] Original steps: {original_steps}")
            
            # Apply intelligent step reduction based on research for all step ranges
            # Research shows that many steps can be reduced with minimal quality loss
            if original_steps > 40:
                # Very high step counts - aggressive reduction
                optimized_steps = max(20, int(original_steps * 0.5))
                debug_print(f"Very high step count: {original_steps} -> {optimized_steps} steps (50% reduction)", debug)
            elif original_steps > 30:
                # High step counts - significant reduction
                optimized_steps = max(15, int(original_steps * 0.6))
                debug_print(f"High step count: {original_steps} -> {optimized_steps} steps (40% reduction)", debug)
            elif original_steps > 20:
                # Medium step counts - moderate reduction
                optimized_steps = max(12, int(original_steps * 0.7))
                debug_print(f"Medium step count: {original_steps} -> {optimized_steps} steps (30% reduction)", debug)
            elif original_steps > 15:
                # Lower step counts - conservative reduction
                optimized_steps = max(10, int(original_steps * 0.75))
                debug_print(f"Lower step count: {original_steps} -> {optimized_steps} steps (25% reduction)", debug)
            elif original_steps > 10:
                # Low step counts - gentle reduction
                optimized_steps = max(8, int(original_steps * 0.8))
                debug_print(f"Low step count: {original_steps} -> {optimized_steps} steps (20% reduction)", debug)
            elif original_steps > 6:
                # Very low step counts - minimal reduction
                optimized_steps = max(5, int(original_steps * 0.85))
                debug_print(f"Very low step count: {original_steps} -> {optimized_steps} steps (15% reduction)", debug)
            else:
                # Extremely low step counts - ultra-minimal reduction
                optimized_steps = max(4, int(original_steps * 0.9))
                debug_print(f"Extremely low step count: {original_steps} -> {optimized_steps} steps (10% reduction)", debug)
            
            # Apply the optimized step count
            if optimized_steps < original_steps:
                p.steps = optimized_steps
                p.speedboost_original_steps = original_steps
                p.speedboost_step_reduction = original_steps - optimized_steps
                
                # Update shared state sampling steps to fix batch progress calculation
                if hasattr(shared, 'state') and hasattr(shared.state, 'sampling_steps'):
                    shared.state.sampling_steps = optimized_steps
                    debug_print(f"Updated shared.state.sampling_steps to {optimized_steps}", debug)
                
                # Calculate estimated speedup
                speedup_factor = original_steps / optimized_steps
                debug_print(f"Step optimization applied: {speedup_factor:.1f}x theoretical speedup", debug)
                
                # Try to apply better scheduler for reduced steps
                self._optimize_scheduler_for_reduced_steps(p, debug)
            else:
                debug_print("No step reduction applied (already at minimum)", debug)
                
        except Exception as e:
            debug_print(f"Dynamic step optimization failed: {e}", debug)
    
    def _optimize_scheduler_for_reduced_steps(self, p: StableDiffusionProcessing, debug: bool):
        """Optimize scheduler settings for reduced step counts"""
        try:
            # Check if we can access the sampler
            if hasattr(p, 'sampler_name'):
                current_sampler = p.sampler_name
                
                # Recommend better samplers for reduced steps based on research
                fast_samplers = ['DPM++ 2M Karras', 'DPM++ SDE Karras', 'Euler a', 'DDIM']
                
                if current_sampler not in fast_samplers:
                    # Don't change sampler automatically, just log recommendation
                    debug_print(f"Current sampler: {current_sampler}. Consider using DPM++ 2M Karras for better quality with fewer steps", debug)
                else:
                    debug_print(f"Using efficient sampler: {current_sampler}", debug)
            
            # Adjust CFG scale for reduced steps if needed
            if hasattr(p, 'cfg_scale') and p.cfg_scale > 10:
                # High CFG scales can be reduced slightly with fewer steps
                original_cfg = p.cfg_scale
                p.cfg_scale = max(7.0, p.cfg_scale * 0.9)
                debug_print(f"Adjusted CFG scale: {original_cfg} -> {p.cfg_scale}", debug)
                
        except Exception as e:
            debug_print(f"Scheduler optimization failed: {e}", debug)
    
    def _apply_conservative_step_optimization(self, p: StableDiffusionProcessing, debug: bool):
        """Apply gentler step optimization for conservative mode"""
        try:
            debug_print("Applying conservative step optimization...", debug)
            
            # Get current step count
            original_steps = getattr(p, 'steps', 20)
            
            # Apply intelligent step reduction for any step count
            # Adaptive reduction based on step count to maintain quality while improving speed
            if original_steps > 40:
                # Significant reduction for very high counts (40+)
                optimized_steps = max(25, int(original_steps * 0.75))
                debug_print(f"Conservative mode: High step count {original_steps} -> {optimized_steps} steps (25% reduction)", debug)
            elif original_steps > 30:
                # Moderate reduction for medium-high step counts (31-40)
                optimized_steps = max(20, int(original_steps * 0.8))
                debug_print(f"Conservative mode: Medium-high step count {original_steps} -> {optimized_steps} steps (20% reduction)", debug)
            elif original_steps >= 25:
                # Gentle reduction for medium step counts (25-30)
                optimized_steps = max(18, int(original_steps * 0.85))
                debug_print(f"Conservative mode: Medium step count {original_steps} -> {optimized_steps} steps (15% reduction)", debug)
            elif original_steps >= 20:
                # Light reduction for lower-medium step counts (20-24)
                optimized_steps = max(15, int(original_steps * 0.9))
                debug_print(f"Conservative mode: Lower-medium step count {original_steps} -> {optimized_steps} steps (10% reduction)", debug)
            elif original_steps >= 15:
                # Very light reduction for low step counts (15-19)
                optimized_steps = max(12, int(original_steps * 0.92))
                debug_print(f"Conservative mode: Low step count {original_steps} -> {optimized_steps} steps (8% reduction)", debug)
            elif original_steps >= 10:
                # Minimal reduction for very low step counts (10-14)
                optimized_steps = max(8, int(original_steps * 0.95))
                debug_print(f"Conservative mode: Very low step count {original_steps} -> {optimized_steps} steps (5% reduction)", debug)
            else:
                # Ultra-minimal reduction for extremely low step counts (below 10)
                optimized_steps = max(6, int(original_steps * 0.97))
                debug_print(f"Conservative mode: Extremely low step count {original_steps} -> {optimized_steps} steps (3% reduction)", debug)
            
            # Apply the optimized step count only if there's a meaningful reduction
            if optimized_steps < original_steps:
                p.steps = optimized_steps
                p.speedboost_original_steps = original_steps
                p.speedboost_step_reduction = original_steps - optimized_steps
                
                # Update shared state sampling steps to fix batch progress calculation
                if hasattr(shared, 'state') and hasattr(shared.state, 'sampling_steps'):
                    shared.state.sampling_steps = optimized_steps
                    debug_print(f"Updated shared.state.sampling_steps to {optimized_steps}", debug)
                
                # Calculate estimated speedup
                speedup_factor = original_steps / optimized_steps
                debug_print(f"Conservative step optimization applied: {speedup_factor:.1f}x theoretical speedup", debug)
                
                # Apply scheduler optimization for reduced steps
                self._optimize_scheduler_for_reduced_steps(p, debug)
            else:
                debug_print("Conservative mode: No step optimization needed", debug)
                
        except Exception as e:
            debug_print(f"Conservative step optimization failed: {e}", debug)
    
    def process_batch(self, p: StableDiffusionProcessing, *args, **kwargs):
        """Called for every batch to ensure optimizations persist throughout batch processing"""
        try:
            # Get debug setting
            debug = False
            if hasattr(p, 'speedboost_settings') and p.speedboost_settings is not None:
                debug = p.speedboost_settings.get('debug', False)
                
            # Ensure memory optimizations are maintained for each batch
            if hasattr(p, 'speedboost_settings') and p.speedboost_settings is not None:
                settings = p.speedboost_settings
                
                # Re-apply memory optimization if it was enabled
                if settings.get('memory', False):
                    self._apply_memory_optimization(p, debug)
                    
                # Ensure inference mode is still active if it was enabled
                if settings.get('inference_mode', False) and not _inference_mode_enabled:
                    enable_inference_mode_globally()
                    
                # Force garbage collection between batches to prevent OOM
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                if debug:
                    batch_number = kwargs.get('batch_number', 'unknown')
                    print(f"[SPEEDBOOST DEBUG] Batch {batch_number}: Memory optimizations maintained")
                    
        except Exception as e:
            logger.warning(f"Speed Boost: process_batch failed: {e}")
    
    def postprocess_batch(self, p: StableDiffusionProcessing, *args, **kwargs):
        """Called after each batch to maintain optimizations and prevent memory issues"""
        try:
            # Get debug setting
            debug = False
            if hasattr(p, 'speedboost_settings') and p.speedboost_settings is not None:
                debug = p.speedboost_settings.get('debug', False)
                
            # Get images from kwargs
            images = kwargs.get('images', None)
            
            # Force cleanup between batches to prevent OOM accumulation
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # Force garbage collection
            gc.collect()
            
            if debug:
                batch_number = kwargs.get('batch_number', 'unknown')
                image_count = len(images) if images is not None else 'unknown'
                print(f"[SPEEDBOOST DEBUG] Batch {batch_number}: Post-batch cleanup completed for {image_count} images")
                
        except Exception as e:
            logger.warning(f"Speed Boost: postprocess_batch failed: {e}")
    
    def postprocess(self, p: StableDiffusionProcessing, processed, *args):
        """Clean up after processing and calculate performance improvements with enhanced error handling"""
        
        try:
            # Validate inputs to prevent NoneType errors
            if p is None:
                logger.warning("Speed Boost: Processing object is None in postprocess")
                return
            
            if processed is None:
                logger.warning("Speed Boost: Processed object is None in postprocess")
                # Don't return here, continue with cleanup
            
            # Calculate generation time
            if self.start_time is not None:
                generation_time = time.time() - self.start_time
                self._update_performance_stats(p, generation_time)
        
        except Exception as timing_e:
            logger.warning(f"Speed Boost: Performance timing failed: {timing_e}")
        
        if self.optimization_active:
            try:
                # Get debug setting before cleanup
                debug = False
                if hasattr(p, 'speedboost_settings') and p.speedboost_settings is not None:
                    debug = p.speedboost_settings.get('debug', False)
                
                # Clean up inference mode if it was enabled
                if hasattr(p, 'speedboost_inference_mode') and p.speedboost_inference_mode:
                    disable_inference_mode_globally()
                
                # Restore original model precision to prevent type mismatches
                # Only restore if FP16 was actually applied
                if hasattr(p, 'speedboost_settings') and p.speedboost_settings is not None and p.speedboost_settings.get('fp16', False):
                    self._restore_original_precision(debug)
                
                # Clean up any temporary optimizations if needed
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                self.optimization_active = False
                
                # Remove speedboost flags from processing object safely
                try:
                    if hasattr(p, 'speedboost_inference_mode'):
                        delattr(p, 'speedboost_inference_mode')
                except AttributeError:
                    pass
                
                try:
                    if hasattr(p, 'speedboost_settings'):
                        delattr(p, 'speedboost_settings')
                except AttributeError:
                    pass
                
                try:
                    if hasattr(p, 'speedboost_cuda_graphs'):
                        delattr(p, 'speedboost_cuda_graphs')
                except AttributeError:
                    pass
                
                if debug:
                    logger.info("Speed Boost: Final cleanup completed successfully")
                
            except Exception as e:
                logger.warning(f"Speed Boost: Cleanup failed: {e}")
                # Ensure optimization_active is reset even if cleanup fails
                self.optimization_active = False
    
    def _update_performance_stats(self, p: StableDiffusionProcessing, generation_time: float):
        """Update performance statistics"""
        try:
            self.performance_stats['total_generations'] += 1
            
            if hasattr(p, 'speedboost_settings'):
                self.performance_stats['optimized_generations'] += 1
                
                # Estimate baseline time (this is approximate)
                # In a real scenario, you'd want to benchmark without optimizations
                estimated_baseline = generation_time * 1.8  # Assume ~80% speedup on average
                speedup_factor = estimated_baseline / generation_time if generation_time > 0 else 1.0
                time_saved = estimated_baseline - generation_time
                
                # Update statistics
                if speedup_factor > self.performance_stats['best_speedup']:
                    self.performance_stats['best_speedup'] = speedup_factor
                
                self.performance_stats['total_time_saved'] += max(0, time_saved)
                
                # Calculate running average speedup
                current_avg = self.performance_stats['average_speedup']
                n = self.performance_stats['optimized_generations']
                self.performance_stats['average_speedup'] = ((current_avg * (n-1)) + speedup_factor) / n
                
                # Log performance if debug is enabled
                if hasattr(p, 'speedboost_settings') and p.speedboost_settings.get('debug', False):
                    debug_print(f"Generation completed in {generation_time:.2f}s (estimated {speedup_factor:.1f}x speedup)", True)
                    
                    # Show step reduction info if applied
                    if hasattr(p, 'speedboost_step_reduction'):
                        debug_print(f"Step reduction: {p.speedboost_step_reduction} steps saved", True)
                    
                    # Show compilation status
                    if hasattr(p, 'speedboost_torch_compiled'):
                        debug_print("Torch compilation was active for this generation", True)
                    
                    debug_print(f"Performance stats: {self.performance_stats['optimized_generations']} optimized generations, "
                              f"avg speedup: {self.performance_stats['average_speedup']:.1f}x, "
                              f"best: {self.performance_stats['best_speedup']:.1f}x, "
                              f"total time saved: {self.performance_stats['total_time_saved']:.1f}s", True)
                
        except Exception as e:
             logger.warning(f"Speed Boost: Performance tracking failed: {e}")
    
    def _get_performance_html(self) -> str:
        """Generate HTML for performance statistics display"""
        stats = self.performance_stats
        
        if stats['total_generations'] == 0:
            return """
            <div style='padding: 15px; background-color: #1f2937; border-radius: 8px; margin-top: 10px;'>
                <h4 style='color: #10b981; margin: 0 0 8px 0;'>ðŸ“Š Performance Statistics</h4>
                <p style='margin: 0; color: #9ca3af;'>No generations completed yet. Statistics will appear after using Speed Boost.</p>
            </div>
            """
        
        optimized_pct = (stats['optimized_generations'] / stats['total_generations']) * 100
        
        return f"""
        <div style='padding: 15px; background-color: #1f2937; border-radius: 8px; margin-top: 10px;'>
            <h4 style='color: #10b981; margin: 0 0 12px 0;'>ðŸ“Š Performance Statistics</h4>
            <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 15px; color: #d1d5db;'>
                <div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #60a5fa;'>Total Generations:</span> <strong>{stats['total_generations']}</strong>
                    </div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #60a5fa;'>Optimized:</span> <strong>{stats['optimized_generations']}</strong> ({optimized_pct:.1f}%)
                    </div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #60a5fa;'>Average Speedup:</span> <strong>{stats['average_speedup']:.1f}x</strong>
                    </div>
                </div>
                <div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #34d399;'>Best Speedup:</span> <strong>{stats['best_speedup']:.1f}x</strong>
                    </div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #34d399;'>Time Saved:</span> <strong>{stats['total_time_saved']:.1f}s</strong>
                    </div>
                    <div style='margin-bottom: 8px;'>
                        <span style='color: #34d399;'>Avg per Gen:</span> <strong>{stats['total_time_saved']/max(1, stats['optimized_generations']):.1f}s</strong>
                    </div>
                </div>
            </div>
            <div style='margin-top: 12px; padding-top: 12px; border-top: 1px solid #374151;'>
                <p style='margin: 0; font-size: 0.9em; color: #9ca3af;'>
                    ðŸ’¡ Speed improvements are estimated based on typical optimization gains. 
                    Actual results may vary depending on your hardware and settings.
                </p>
            </div>
        </div>
        """

# Register the script
def on_script_unloaded():
    """Cleanup when script is unloaded"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass

script_callbacks.on_script_unloaded(on_script_unloaded)

# Global variable to track inference mode state
_inference_mode_enabled = False
_original_forward_methods = {}

def enable_inference_mode_globally():
    """Enable inference mode for model forward passes"""
    global _inference_mode_enabled, _original_forward_methods
    
    if _inference_mode_enabled:
        return
    
    try:
        # Patch the model's forward method to use inference mode
        if hasattr(shared.sd_model, 'model') and hasattr(shared.sd_model.model, 'diffusion_model'):
            diffusion_model = shared.sd_model.model.diffusion_model
            
            if hasattr(diffusion_model, 'forward') and 'diffusion_model' not in _original_forward_methods:
                original_forward = diffusion_model.forward
                _original_forward_methods['diffusion_model'] = original_forward
                
                def inference_mode_forward(*args, **kwargs):
                    with torch.inference_mode():
                        return original_forward(*args, **kwargs)
                
                diffusion_model.forward = inference_mode_forward
                _inference_mode_enabled = True
                logger.info("Speed Boost: Inference mode enabled globally")
                
    except Exception as e:
        logger.warning(f"Speed Boost: Failed to enable inference mode: {e}")

def disable_inference_mode_globally():
    """Disable inference mode and restore original methods"""
    global _inference_mode_enabled, _original_forward_methods
    
    if not _inference_mode_enabled:
        return
    
    try:
        # Restore original forward methods
        if hasattr(shared.sd_model, 'model') and hasattr(shared.sd_model.model, 'diffusion_model'):
            diffusion_model = shared.sd_model.model.diffusion_model
            
            if 'diffusion_model' in _original_forward_methods:
                diffusion_model.forward = _original_forward_methods['diffusion_model']
                del _original_forward_methods['diffusion_model']
                
        _inference_mode_enabled = False
        logger.info("Speed Boost: Inference mode disabled")
        
    except Exception as e:
        logger.warning(f"Speed Boost: Failed to disable inference mode: {e}")

# Enhanced denoiser callback
def speedboost_denoiser_callback(params: CFGDenoiserParams):
    """Apply optimizations during denoising"""
    try:
        # Check if speedboost is enabled for this generation
        if hasattr(params, 'p') and hasattr(params.p, 'speedboost_inference_mode'):
            if params.p.speedboost_inference_mode and not _inference_mode_enabled:
                enable_inference_mode_globally()
        
        # Fix sampling steps for progress calculation if step optimization was applied
        if hasattr(params, 'p') and hasattr(params.p, 'speedboost_settings'):
            p = params.p
            if (hasattr(p, 'speedboost_original_steps') and 
                hasattr(shared, 'state') and hasattr(shared.state, 'sampling_steps')):
                # Only update on the first denoiser call to avoid repeated updates
                if shared.state.sampling_step == 0 and shared.state.sampling_steps != p.steps:
                    shared.state.sampling_steps = p.steps
                    if hasattr(p, 'speedboost_settings') and p.speedboost_settings.get('debug', False):
                        print(f"[SPEEDBOOST DEBUG] Denoiser callback: Updated shared.state.sampling_steps from {shared.state.sampling_steps} to {p.steps}")
    except Exception as e:
        logger.warning(f"Speed Boost: Denoiser callback failed: {e}")

# Register the denoiser callback
script_callbacks.on_cfg_denoiser(speedboost_denoiser_callback)